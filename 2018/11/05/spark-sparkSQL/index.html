<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>spark SQL 编程入门 | ljming的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="好久没有更新 java spark 学习了，今天趁着没什么事，把 sparkSQL 的学习补上。">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="spark SQL 编程入门">
<meta property="og:url" content="https://ljm516.github.io/2018/11/05/spark-sparkSQL/index.html">
<meta property="og:site_name" content="ljming的博客">
<meta property="og:description" content="好久没有更新 java spark 学习了，今天趁着没什么事，把 sparkSQL 的学习补上。">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2018-11-05T15:22:52.622Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark SQL 编程入门">
<meta name="twitter:description" content="好久没有更新 java spark 学习了，今天趁着没什么事，把 sparkSQL 的学习补上。">
  
    <link rel="alternate" href="/atom.xml" title="ljming的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ljming的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">程序人生</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ljm516.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark-sparkSQL" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/05/spark-sparkSQL/" class="article-date">
  <time datetime="2018-11-05T15:22:52.622Z" itemprop="datePublished">2018-11-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      spark SQL 编程入门
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>好久没有更新 java spark 学习了，今天趁着没什么事，把 sparkSQL 的学习补上。</p>
<a id="more"></a>
<h2 id="在本地提交-spark-任务"><a href="#在本地提交-spark-任务" class="headerlink" title="在本地提交 spark 任务"></a>在本地提交 spark 任务</h2><p><code>spark-submit --master local --class top.ljming.javaSparkLearn.sparkSQL.SparkSQLApplication --jars --driver-class-path java-spark-learn-1.0-SNAPSHOT.jar</code></p>
<h2 id="SparkSession-的简单入门"><a href="#SparkSession-的简单入门" class="headerlink" title="SparkSession 的简单入门"></a>SparkSession 的简单入门</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dataFrameActions</span><span class="params">(Dataset df)</span> </span>&#123;</span><br><span class="line">    df.show();</span><br><span class="line">    <span class="comment">// 以树形结构打印 schema</span></span><br><span class="line">    df.printSchema();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 选择 `name` 列</span></span><br><span class="line">    System.out.println(<span class="string">"df.show() second call"</span>);</span><br><span class="line">    df.show();</span><br><span class="line">    df.select(<span class="string">"name"</span>).show();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 选择所有的数据，但对 `age` 列执行 +1</span></span><br><span class="line">    System.out.println(<span class="string">"df.show() third call"</span>);</span><br><span class="line">    df.show();</span><br><span class="line">    df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 选择 `age` 大于 21 的 people</span></span><br><span class="line">    df.select(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据 `age` 分组并计算</span></span><br><span class="line">    df.groupBy(<span class="string">"age"</span>).count().show();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在运行这个程序的时候，有个问题一直困扰这我，就是 <code>df.show()</code> 这个方法，我在 idea 里面执行时，总会出现 <code>java.lang.NoSuchMethodError: org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback()Lscala/Option;</code> 这个错。起初我也是 spark 版本的问题，现在换成了 2.2 版本还是一样。今天我把项目打成 jar 包，然后扔到我的阿里云服务器（部署好 spark 环境）上去，通过 spark-submit 的方式运行，跑到贼溜。然后，我回到本机，在 cmd 窗口以 spark-submit 的方式提交运行，和在服务器一样，跑的贼溜。WTF，还没搞清楚什么情况，初步猜测是 idea 设置的 spark 运行环境有问题。</p>
<h2 id="程序化运行-SQL"><a href="#程序化运行-SQL" class="headerlink" title="程序化运行 SQL"></a>程序化运行 SQL</h2><p><code>sparksession</code> 上的sql函数使应用程序能够以编程方式运行sql查询，并将结果作为 <code>Dataset&lt;row&gt;</code> 返回</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">selectBySQL</span><span class="params">(Dataset df, SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"people"</span>); <span class="comment">// 注册 DataFrame 为一个 SQL 的临时视图</span></span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; sqlDF = sparkSession.sql(<span class="string">"select * from people"</span>);</span><br><span class="line"></span><br><span class="line">    sqlDF.show();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="全局临时视图"><a href="#全局临时视图" class="headerlink" title="全局临时视图"></a>全局临时视图</h2><p>Spark SQL 中的临时视图是会话范围的，如果创建它的会话终止，将会消失。如果想要在所有会话之间共享一个临时视图，并在Spark应用程序终止之前保持活动状态，<br>则可以创建一个全局临时视图。全局临时视图绑定到系统保存的数据库 <code>global_temp</code>。我们必须使用合格的名称来引用它，比如：<code>select * from global_temp.view1</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// global temporary view</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">globalTempView</span><span class="params">(Dataset df, SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 注册 DataFrame 作为一个全局临时视图</span></span><br><span class="line">        df.createGlobalTempView(<span class="string">"people"</span>);</span><br><span class="line">        <span class="comment">// 全局临时视图被绑定到系统保留的数据库'global_temp'</span></span><br><span class="line">        sparkSession.sql(<span class="string">"select * from global_temp.people"</span>).show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 全局临时视图是跨 session 域的</span></span><br><span class="line">        sparkSession.newSession().sql(<span class="string">"select * from global_temp.people"</span>).show();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (AnalysisException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="创建-Datasets"><a href="#创建-Datasets" class="headerlink" title="创建 Datasets"></a>创建 Datasets</h2><p>dataset 类似于 RDDs，但是，Datasets使用了一个专门的编码器Encoder来序列化对象而不是使用Java的序列化或Kryo。但是不管是 Encoders 还是 标准的序列化工具，他们都负责把对象转成字节。Encoder 是动态生成的代码，它使用的格式允许 Spark 执行像过滤（filter）、排序（sorting）和 hashing等操作而不需要将对象反序列化成对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 创建 datasets</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createDatasets</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    Person person = <span class="keyword">new</span> Person();</span><br><span class="line">    person.setAge(<span class="number">25</span>);</span><br><span class="line">    person.setCompany(<span class="string">"任子行"</span>);</span><br><span class="line">    person.setGender(<span class="string">"female"</span>);</span><br><span class="line">    person.setAddress(<span class="string">"wuhan"</span>);</span><br><span class="line">    person.setName(<span class="string">"chenyang"</span>);</span><br><span class="line"></span><br><span class="line">    Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</span><br><span class="line">    Dataset&lt;Person&gt; javaBeanDS = sparkSession.createDataset(Collections.singletonList(person), personEncoder);</span><br><span class="line">    javaBeanDS.show();</span><br><span class="line"></span><br><span class="line">    Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</span><br><span class="line">    Dataset&lt;Integer&gt; primitiveDS = sparkSession.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), integerEncoder);</span><br><span class="line">    Dataset&lt;Integer&gt; transformedDS = primitiveDS.map((MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>, integerEncoder);</span><br><span class="line">    System.out.println(transformedDS.collect());</span><br><span class="line"></span><br><span class="line">    String path = <span class="string">"D:\\lijiangming\\docs\\sparkLearn\\sparkSQLExample.json"</span>;</span><br><span class="line">    Dataset&lt;Person&gt; peopleDS = sparkSession.read().json(path).as(personEncoder);</span><br><span class="line">    peopleDS.show();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="和-RDD-的相互转换"><a href="#和-RDD-的相互转换" class="headerlink" title="和 RDD 的相互转换"></a>和 RDD 的相互转换</h2><p>Spark SQL 支持两个不同的 RDD 和 Datasets 的转换方式。</p>
<p>第一种方法使用反射来推断包含特定类型对象的 rdd 的模式。在知道 schema 的时候，这种基于反射的方法使得代码更加简洁，并且Spark应用程序运行效果也更好。<br>第二种创建 Datasets 的方法是通过编程接口，允许您构建一个模式，然后将其应用于现有的 rdd。而这个方法更加详细，它允许程序在运行的时候才知道 <code>columns</code> 和它们的类型的情况下构建数据集。</p>
<h3 id="使用反射来推断-Schema"><a href="#使用反射来推断-Schema" class="headerlink" title="使用反射来推断 Schema"></a>使用反射来推断 Schema</h3><p>Spark SQL 支持自动将 JavaBeans 的 RDD 转换成 DataFrame。JavaBean 的信息，使用反射获取，定义 table 的约束。目前，Spark SQL 不支持的 JavaBean 包含 Map。嵌套的 JavaBean 和 List 或者 Array 是支持的。您可以通过创建一个实现可序列化的类来创建一个 Javabean，并为其所有字段设置 getter 和 setter。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用反射推断 Scheme</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">InferSchemaByReflect</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    String path = <span class="string">"D:\\lijiangming\\docs\\sparkLearn\\sparkSQLExample.txt"</span>;</span><br><span class="line">    <span class="comment">// 从一个 txt 文件中创建 RDD</span></span><br><span class="line">    JavaRDD&lt;Person&gt; personJavaRDD = sparkSession.read().textFile(path).javaRDD().map(line -&gt; LearnUtils.buildPerson(line));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  将 schema 应用于 Javabeans 的 rdd 以获取 datasets</span></span><br><span class="line">    Dataset personDF = sparkSession.createDataFrame(personJavaRDD, Person.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 DataFrame 注册为一个临时视图</span></span><br><span class="line">    personDF.createOrReplaceTempView(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SQL 可以通过 Spark 提供的 sql 方法来执行</span></span><br><span class="line">    Dataset&lt;Row&gt; selectPersonDF = sparkSession.sql(<span class="string">"select name from person where age between 20 and 28"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每一行的每一列数据，可以通过下标的方式</span></span><br><span class="line">    Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">    Dataset&lt;String&gt; selectPersonNameByIndexDF = selectPersonDF.map((MapFunction&lt;Row, String&gt;) row -&gt;</span><br><span class="line">            <span class="string">"name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>), stringEncoder);</span><br><span class="line"></span><br><span class="line">    selectPersonNameByIndexDF.show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="编程指定-schema"><a href="#编程指定-schema" class="headerlink" title="编程指定 schema"></a>编程指定 schema</h3><p>当不能提前定义 Javabean类时（例如，记录的结构是用字符串编码的，或者文本数据集将被解析，字段对不同的用户来说映射会不同），<code>Dataset&lt;row&gt;</code> 以编程方式创建有三个步骤。</p>
<ul>
<li>通过原来的RDD创建一个Rows格式的RDD</li>
<li>创建以StructType表现的schema，该StructType与步骤1创建的Rows结构RDD相匹配</li>
<li>通过SparkSession的createDataFrame方法对Rows格式的RDD指定schema</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用编程的方式指定 schema</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">inferSchemaByProgram</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    String path = <span class="string">"D:\\lijiangming\\docs\\sparkLearn\\sparkSQLExample.txt"</span>;</span><br><span class="line">    JavaRDD&lt;String&gt; personRDD = sparkSession.sparkContext().textFile(path, <span class="number">1</span>).toJavaRDD();</span><br><span class="line"></span><br><span class="line">    String schemaString = <span class="string">"name age"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 基于 schemaString 生成 schema</span></span><br><span class="line">    List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">        StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">        fields.add(field);</span><br><span class="line">    &#125;</span><br><span class="line">    StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 rdd 的结果转成 Row</span></span><br><span class="line">    JavaRDD&lt;Row&gt; rowRdd = personRDD.map((Function&lt;String, Row&gt;) s -&gt; &#123;</span><br><span class="line">        String[] attributes = s.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 schema 应用到 rdd</span></span><br><span class="line">    Dataset&lt;Row&gt; personDataFrame = sparkSession.createDataFrame(rowRdd, schema);</span><br><span class="line"></span><br><span class="line">    personDataFrame.createOrReplaceTempView(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; results = sparkSession.sql(<span class="string">"select name from people"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SQL 查询的结果是 DataFrame，并支持所有正常的 rdd 操作</span></span><br><span class="line">    Dataset&lt;String&gt; namesDS = results.map((MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"name: "</span> + row.getString(<span class="number">0</span>), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">    namesDS.show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>Spark SQL 通过 DataFrame 的接口，支持多种数据源的操作。一个数据源可以是用来进行关系型转换或者创建临时视图。将一个 DataFrame 注册成为临时视图，允许你运行 SQL 语句来查询它的数据。这里首先介绍使用 Spark 数据源加载和保存数据的通用方法，然后对内置数据源进行详细介绍。</p>
<h3 id="通用-Load-Save-方法"><a href="#通用-Load-Save-方法" class="headerlink" title="通用 Load / Save 方法"></a>通用 Load / Save 方法</h3><p>在最简单的形式中，默认的数据源（parquet，除非另外配置spark.sql.sources.default）将用于所有的操作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; userDF = sparkSession.read().load(<span class="string">"file_path"</span>);</span><br><span class="line">userDF.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write().save(<span class="string">"namesAndFavoriteColors.parquet"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>您还可以手动指定将要使用的数据源以及您想要传递给数据源的其他选项。数据源可以通过全限定名指定(例如: org.apache.spark.sql.parquet), 但是对于内置的源代码，你也可以使用它们的短名称(json, parquet, jdbc, orc, libsvm, csv, text). 从任意类型的数据源加载的 DataFrames 使用它的语法转换成其它类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; personDF = sparkSession.read().format(<span class="string">"json"</span>).load(<span class="string">"file_path"</span>);</span><br><span class="line">personDF.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write().format(<span class="string">"parquet"</span>).save(<span class="string">"nameAndAges.parquet"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="在文件上直接运行-SQL"><a href="#在文件上直接运行-SQL" class="headerlink" title="在文件上直接运行 SQL"></a>在文件上直接运行 SQL</h3><p>可以直接用 SQL 查询该文件，而不是使用 read api 的方式，将文件加载到 DataFrame，再查询。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; sqlDF = sparkSession.sql(<span class="string">"select * from parquet. `file_path`"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h3><p>保存操作可以选择保存模式，指定如何处理现有数据(如果存在)。需要注意的是，这些模式不使用任何锁和不是原子操作的。另外，当执行 overwrite时，在写出新数据之前数据将被删除。</p>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>SaveMode.ErrorIfExists (default)</td>
<td>“error” (default)</td>
<td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>
</tr>
</tbody>
</table>
<h3 id="保存到持久化-tables"><a href="#保存到持久化-tables" class="headerlink" title="保存到持久化 tables"></a>保存到持久化 tables</h3><p>也可以使用 <code>saveastable</code> 命令将 DataFrames 作为持久表保存到 Hive Metastore中。请注意，现有的 Hive 部署对使用此功能不是必需的。Spark 会创建一个默认的本地 Hive metastore(shiyong Derby)。和 <code>createOrReplaceTempView</code> 命令不同，<code>saveAsTable</code> 会实例化 DataFrame 里的内容，并创建一个指向 Hive metastore 里数据的指针。即使 Spark 程序重新启动，持久化 Table 也还是存在的，只要你保持连接相同的 metastore，可以通过调用带有表名称的 sparksession 上的表方法来创建持久表的DataFrame。</p>
<p>对于基于文件的数据源，比如，text, parquet, json 等，可以通过路径选项指定一个 custom table。比如： df.write.option(“path”, “/some/path”).saveAsTable(“t”). 当 table 被删除了，custom table 路径不会被删掉，并且表数据也不会被删除。如果不指定 custom table, Spark 程序会把数据写到位于 warehouse 目录下默认 table 路径。当 table 被删除，默认的 table 路径也将被删除。</p>
<p>从 2.1 版本开始，持久化数据源表具有存储在 Hive metestore 中的每个分区数据。这带来了以下几个好处：</p>
<ul>
<li>由于Metastore只能返回查询所需的分区，因此不再需要发现第一个查询的所有分区</li>
<li>Hive ddls 如 <code>alter table partition ... set location</code> 现在可用于使用数据源 api 创建的表。</li>
</ul>
<p>请注意，在创建外部数据源表（具有路径选项的那些表）时，默认情况下不会收集分区信息。要同步 Metastore 中的分区信息，可以调用  <code>msck pepair table</code> 。</p>
<h3 id="parquet-files"><a href="#parquet-files" class="headerlink" title="parquet files"></a>parquet files</h3><p>parquet 是由许多其他数据处理系统支持的柱状格式。spark sql 为阅读和编写自动保存原始数据模式的 parquet 文件提供了支持。当写入 parquet 文件时，由于兼容性的原因，所有列都会自动转换为空值。</p>
<h4 id="程序加载数据"><a href="#程序加载数据" class="headerlink" title="程序加载数据"></a>程序加载数据</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">LoadingDataProgram</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    String path = <span class="string">"D:\\lijiangming\\docs\\sparkLearn\\sparkSQLExample.json"</span>;</span><br><span class="line">    Dataset&lt;Row&gt; personDF = sparkSession.read().json(path);</span><br><span class="line">    <span class="comment">// dataframes 可以保存为 parquet 文件，维护 schema 信息</span></span><br><span class="line">    personDF.write().parquet(<span class="string">"person.parquet"</span>);</span><br><span class="line">    <span class="comment">// 读取 parquet 文件</span></span><br><span class="line">    Dataset&lt;Row&gt; parquetFileDF = sparkSession.read().parquet(<span class="string">"person.parquet"</span>);</span><br><span class="line"></span><br><span class="line">    parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>);</span><br><span class="line">    Dataset&lt;Row&gt; namesDf = sparkSession.sql(<span class="string">"select name from parquetFile where age between 20 and 28"</span>);</span><br><span class="line">    Dataset&lt;String&gt; namesDS = namesDf.map((MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"name: "</span> + row.getString(<span class="number">0</span>), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">    namesDS.show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="分区发现"><a href="#分区发现" class="headerlink" title="分区发现"></a>分区发现</h4><p>表分区是像 Hive 这样的系统常用的优化方法。在分区表里，数据通过不同的分区列被存储在不同的目录里。所有内置文件源，都可以自动发现和推断分区信息。<br>例如：我们可以使用以下目录结构将所有以前使用的人口数据存储到分区表中，并将两个额外的列（性别和国家/地区）作为分区列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>
<p>通过传递 <code>path/to/table</code> 给 SparkSession.read.parquet 或者 SparkSession.read.load，Spark SQL 将自动从 path 中精确获取分区信息。现在返回的 DataFrame 的模式变成了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure>
<p>请注意，分区列的数据类型是自动推断的.目前支持数字数据类型，日期，时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这种情况，自动类型推断可以通过 <code>spark.sql.sources.partitionColumnTypeInference.enabled</code> 配置，它的默认值为 true。当类型推断被禁用，字符串类型将用于分区列。</p>
<p>从 spark 1.6.0 开始，分区发现只在默认情况下在给定路径下找到分区。对于上面的例子，如果用户传递 <code>path/to/table/gender=male</code> 给 SparkSession.read.parquet 或者 SparkSession.read.load, gender 不在看做是分区列。如果用户需要指定基础路径作为分区信息解析的开始路径，可以在数据源可选项中设置 <code>basePath</code>，例如：当数据的路径为  <code>path/to/table/gender=male</code>，用户也设置了 <code>basePath</code> 为 <code>path/to/table/</code>，gender 将会视为分区列。</p>
<h4 id="Scheme-合并"><a href="#Scheme-合并" class="headerlink" title="Scheme 合并"></a>Scheme 合并</h4><p>像 ProtocolBuffer, Avro 和 Thrift，Parquet也支持 Schema 演变。用户可以从一个简单的 Schema 开始，逐渐的添加更多的列到 Schema 中。在这中情况下，用户可能会用不同的但是相互兼容的模式结束多个 Parquet。Parquet 数据源现阶段可以自动检测这种情况并且合并所有这些文件的 schema。</p>
<p>由于 Schema 合并是个很昂贵的操作，而且在大多是的情况下不是必须的。在 1.5.0 版本开始，默认是关闭的。可以通过以下方式打开它:</p>
<ul>
<li>当读取 Parquet 文件时，设置数据源可选项 <code>mergeSchema</code> 为 <code>true</code> 。或者</li>
<li>设置全局的 SQL 可选项 <code>spark.sql.parquet.mergeSchema</code> 为 <code>true</code></li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Schema 合并</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">schemaMerging</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    List&lt;Square&gt; squares = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).stream().forEach(value -&gt; &#123;</span><br><span class="line">        Square square = <span class="keyword">new</span> Square();</span><br><span class="line">        square.setValue(value);</span><br><span class="line">        square.setSquare(value * value);</span><br><span class="line">        squares.add(square);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个简单的 DataFrame，存储到分区文件夹</span></span><br><span class="line">    Dataset&lt;Row&gt; squareDF = sparkSession.createDataFrame(squares, Square.class);</span><br><span class="line">    squareDF.write().parquet(BASE_DIR + <span class="string">"data/test_table/key=1"</span>);</span><br><span class="line"></span><br><span class="line">    List&lt;Cube&gt; cubes = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Arrays.asList(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>).stream().forEach(value -&gt; &#123;</span><br><span class="line">        Cube cube = <span class="keyword">new</span> Cube();</span><br><span class="line">        cube.setValue(value);</span><br><span class="line">        cube.setValue(value * value * value);</span><br><span class="line">        cubes.add(cube);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在另一个分区文件夹中创建另一个 DataFrame；添加一个新 column ，移除掉一个存在的column</span></span><br><span class="line">    Dataset&lt;Row&gt; cubeDF = sparkSession.createDataFrame(cubes, Cube.class);</span><br><span class="line">    cubeDF.write().parquet(BASE_DIR + <span class="string">"data/test_table/key=2"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取分区表</span></span><br><span class="line">    Dataset&lt;Row&gt; mergedDF = sparkSession.read().option(<span class="string">"mergeSchema"</span>, <span class="keyword">true</span>).parquet(BASE_DIR + <span class="string">"data/test_table"</span>);</span><br><span class="line">    mergedDF.printSchema();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="JSON-Datasets"><a href="#JSON-Datasets" class="headerlink" title="JSON Datasets"></a>JSON Datasets</h3><p>Spark SQL 可以自动推测 JSON dataset 的 Schema 并把它作为一个 Dataset<row> 加载。这个转换可以通过 <code>SparkSession.read().json()</code> 实现，不管是一个 Dataset<string> 还是个 JSON 文件。</string></row></p>
<p>需要注意的是提供的文件是符合 JSON 格式的，而并不一定是 JSON 文件。每行必须包含一个单独的，自包含的有效json对象。</p>
<p>对于常规的多行JSON文件，将多行选项设置为true。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JSON dataset</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">JSONDataset</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 文件路径可以是单个的文本文件，也可以是一个存储了文本文件的文件夹</span></span><br><span class="line">    Dataset&lt;Row&gt; personDF = sparkSession.read().json(BASE_DIR + <span class="string">"sparkSQLExample.json"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可推测的 schema 通过 printSchema() 方法显示出来</span></span><br><span class="line">    personDF.printSchema();</span><br><span class="line"></span><br><span class="line">    personDF.createOrReplaceTempView(<span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; namesDF = sparkSession.sql(<span class="string">"select name from person where age between 20 and 28"</span>);</span><br><span class="line">    namesDF.show();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 或者，可以通过一个 JSON格式的字符串创建一个 DataFrame</span></span><br><span class="line">    String jsonData = <span class="string">"&#123;\"name\":\"tracy\",\"address\":\"Houston\",\"gender\":\"male\",\"company\":\"Amzon\",\"age\":\"32\"&#125;"</span>;</span><br><span class="line">    Dataset&lt;Row&gt; anotherPerson = sparkSession.read().json(jsonData.toString());</span><br><span class="line">    anotherPerson.show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Hive-Tables"><a href="#Hive-Tables" class="headerlink" title="Hive Tables"></a>Hive Tables</h3><p>Spark SQL 也支持读写存储在 Apache Hive 中的数据。但是，由于 Hive 需要大量的依赖，这些依赖在默认的 Spark 分布中不包含。如果 Hive 的依赖可以在 classpath 找到，Spark 就会自动加载它们。请注意，这些 Hive 的依赖也必须存在于所有工作节点上，因为它们需要访问 Hive 序列化和反序列化库以访问存储在 Hive 表里的数据。</p>
<p>通过在<code>conf/</code>中放置 <code>hive-site.xml</code>，<code>core-site.xml</code>（用于安全配置）和 <code>hdfs-site.xml</code>（用于hdfs配置）文件来完成 Hive 配置。</p>
<p>在使用 Hive 时，必须实例化 Hive 支持，包括连接到持久性 Hive Metastore，支持 Hive serdes和 Hive 用户定义的功能。没有部署 Hive 的用户仍然可以启用 Hive 支持。当未通过 <code>hive-site.xml</code> 配置时，上下文会自动在当前目录中创建 <code>metastore_db</code>，并创建一个由 <code>spark.sql.warehouse.dir</code> 配置的目录，该目录默认为 spark 应用程序的当前目录中的 <code>spark-warehouse</code> 目录开始。需要注意的是，<code>hive-site.xml</code> 中的 <code>hive.metastore.warehouse.dir</code> 从 2.0.0 版本开始被弃用了。作为替代，使用 <code>spark.sql.warehouse.dir</code> 指定 warehouse 中的 database 的地址。您可能需要将写权限授予启动 Spark 应用程序的用户。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> top.ljming.javaSparkLearn.sparkSQL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> top.ljming.javaSparkLearn.model.Record;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkHiveApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String BASE_DIR = <span class="string">"D:\\lijiangming\\docs\\sparkLearn\\"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> SparkSession <span class="title">initSparkSession</span><span class="params">(String warehouseLocation)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SparkSession.builder().appName(SparkHiveApplication.class.getSimpleName())</span><br><span class="line">                .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation).enableHiveSupport().getOrCreate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</span><br><span class="line">        System.out.println(<span class="string">"warehouseLocation: ------&gt;"</span> + warehouseLocation);</span><br><span class="line">        SparkSession sparkSession = initSparkSession(warehouseLocation);</span><br><span class="line">        hiveTable(sparkSession);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// HiveTable</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">hiveTable</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">        sparkSession.sql(<span class="string">"create table if not exists src (key int, value string) using hive"</span>);</span><br><span class="line">        String sql = <span class="string">"load data local inpath 'kv1.txt' into table src"</span>;</span><br><span class="line">        System.out.println(<span class="string">"load data sql: ---&gt; "</span> + sql);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            sparkSession.sql(sql);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.out.println(e.toString());</span><br><span class="line">            System.out.println(e.getMessage());</span><br><span class="line">            System.exit(-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查询以 hiveQL 表示</span></span><br><span class="line">        sparkSession.sql(<span class="string">"select * from src"</span>).show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 也支持聚合查询</span></span><br><span class="line">        sparkSession.sql(<span class="string">"select count(*) from src"</span>).show();</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; sqlDF = sparkSession.sql(<span class="string">"select key, value from src where key &lt; 10 order by key"</span>);</span><br><span class="line">        Dataset&lt;String&gt; stringDF = sqlDF.map((MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"key: "</span> + row.get(<span class="number">0</span>)</span><br><span class="line">                + <span class="string">", value: "</span> + row.get(<span class="number">1</span>), Encoders.STRING());</span><br><span class="line">        stringDF.show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 也可以结合 SparkSession 使用 DataFrame 去创建临时视图</span></span><br><span class="line">        List&lt;Record&gt; recordList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> key = <span class="number">1</span>; key &lt; <span class="number">100</span>; key++) &#123;</span><br><span class="line">            Record record = <span class="keyword">new</span> Record();</span><br><span class="line">            record.setKey(key);</span><br><span class="line">            record.setValue(<span class="string">"val_"</span> + key);</span><br><span class="line">            recordList.add(record);</span><br><span class="line">        &#125;</span><br><span class="line">        Dataset&lt;Row&gt; recordDF = sparkSession.createDataFrame(recordList, Record.class);</span><br><span class="line">        recordDF.createOrReplaceTempView(<span class="string">"record"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 然后查询可以将 DataFrame 数据与 Hive 中的数据结合起来。</span></span><br><span class="line">        sparkSession.sql(<span class="string">"select * from record r join src s on r.key = s.key"</span>).show();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="指定-Hive-table-的存储格式"><a href="#指定-Hive-table-的存储格式" class="headerlink" title="指定 Hive table 的存储格式"></a>指定 Hive table 的存储格式</h3><p>当你创建 Hive 表时，你需要定义表如何从文件系统中读写数据。例如: <code>input format</code> 和 <code>output format</code>。你也需要定义将数据反序列化成 rows，或者将 rows 序列化成文件数据。例如：<code>serde</code>。下面的可选项可以用来指定存储格式（”serde”, “input format”, “output format”）。例如: <code>create table src(id int) using hive options(fileFormat, &#39;parquet&#39;)</code>。默认的，我们将以纯文本形式读取 table 文件。注意，Hive 存储处理器在建表的时候不支持 <code>yet</code> ，你可以在 hive 端使用存储处理器创建一个表，并使用 spark sql 来读取它。</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>fileFormat</td>
<td>A fileFormat is kind of a package of storage format specifications, including “serde”, “input format” and “output format”. Currently we support 6 fileFormats: ‘sequencefile’, ‘rcfile’, ‘orc’, ‘parquet’, ‘textfile’ and ‘avro’.</td>
</tr>
<tr>
<td>inputFormat, outputFormat</td>
<td>These 2 options specify the name of a corresponding <code>InputFormat</code> and <code>OutputFormat</code> class as a string literal, e.g. <code>org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</code>. These 2 options must be appeared in pair, and you can not specify them if you already specified the <code>fileFormat</code> option.</td>
</tr>
<tr>
<td>serde</td>
<td>This option specifies the name of a serde class. When the <code>fileFormat</code> option is specified, do not specify this option if the given <code>fileFormat</code> already include the information of serde. Currently “sequencefile”, “textfile” and “rcfile” don’t include the serde information and you can use this option with these 3 fileFormats.</td>
</tr>
<tr>
<td>fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td>These options can only be used with “textfile” fileFormat. They define how to read delimited files into rows.</td>
</tr>
</tbody>
</table>
<h3 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h3><p>Spark SQL 也支持使用 JDBC 从其他数据库读取数据作为一个数据源。这个功能使用 JdbcRDD 比较受欢迎。因为它返回的结果时一个 DataFrame，它很容易在 Spark SQL 里做处理或者和其它数据源做 Join 操作。jdbc数据源也更容易在 java 或 python 中使用，因为它不需要用户提供一个对象。（请注意，这与 <code>spark sql jdbc server</code> 不同，后者允许其他应用程序使用spark sql运行查询）</p>
<p>要开始，你将需要在 Spark 类路径中为你的使用的数据库提供 jdbc 驱动程序。例如：在 Spark shell 连接 postgres，你需要运行一下命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --driver-class-path postgresql-9-4-1207.jar --jars postgresql-9-4-1207.jar</span><br></pre></td></tr></table></figure>
<p>远程数据库的表可以通过数据源的 API 加载成为 DataFrame 或者 Spark SQL 的临时视图。用户可以在数据源可选项中指定 JDBC 连接的属性。<code>user</code> 和 <code>password</code> 是连接登录数据源的通常属性。除了连接属性之外，spark 还支持以下不区分大小写的选项:</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&amp;password=secret</td>
</tr>
<tr>
<td>dbtable</td>
<td>The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.</td>
</tr>
<tr>
<td>driver</td>
<td>The class name of the JDBC driver to use to connect to this URL.</td>
</tr>
<tr>
<td>partitionColumn, lowerBound, upperBound</td>
<td>These options must all be specified if any of them is specified. In addition, numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td>
</tr>
<tr>
<td>numPartitions</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing.</td>
</tr>
<tr>
<td>fetchsize</td>
<td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td>
</tr>
<tr>
<td>batchsize</td>
<td>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to 1000.</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of READ_UNCOMMITTED. This option applies only to writing. Please refer the documentation in java.sql.Connection.</td>
</tr>
<tr>
<td>truncate</td>
<td>This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to false. This option applies only to writing.</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). This option applies only to writing.</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: “name CHAR(64), comments VARCHAR(1024)”). The specified types should be valid spark sql data types. This option applies only to writing.</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JDBC to other databases</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">jdbc2OtherDatabases</span><span class="params">(SparkSession sparkSession)</span> </span>&#123;</span><br><span class="line">    Dataset&lt;Row&gt; jdbcDF1 = sparkSession.read()</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</span><br><span class="line">            .option(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>)</span><br><span class="line">            .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</span><br><span class="line">            .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</span><br><span class="line">            .load();</span><br><span class="line"></span><br><span class="line">    Properties connectionProperties = <span class="keyword">new</span> Properties();</span><br><span class="line">    connectionProperties.put(<span class="string">"user"</span>, <span class="string">"username"</span>);</span><br><span class="line">    connectionProperties.put(<span class="string">"password"</span>, <span class="string">"pwd"</span>);</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; jdbcDF2 = sparkSession.read().jdbc(<span class="string">"jdbc.postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存数据到 JDBC 源</span></span><br><span class="line">    jdbcDF1.write()</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>)</span><br><span class="line">            .option(<span class="string">"datable"</span>, <span class="string">"schema.tablename"</span>)</span><br><span class="line">            .option(<span class="string">"user"</span>, <span class="string">"username"</span>)</span><br><span class="line">            .option(<span class="string">"password"</span>, <span class="string">"password"</span>)</span><br><span class="line">            .save();</span><br><span class="line"></span><br><span class="line">    jdbcDF2.write()</span><br><span class="line">            .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写入数据时指定 table 列的数据类型</span></span><br><span class="line">    jdbcDF1.write()</span><br><span class="line">            .option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</span><br><span class="line">            .jdbc(<span class="string">"jdbc:postgresql:dbserver"</span>, <span class="string">"schema.tablename"</span>, connectionProperties);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="故障排除"><a href="#故障排除" class="headerlink" title="故障排除"></a>故障排除</h4><ul>
<li><p>JDBC 驱动对所有的执行器的原始类加载器和客户端 session 都必须可见。这是因为java的 drivermanager 类会执行一个安全检查，导致它在打开连接时忽略原始类加载器不可见的所有驱动程序。用一个很方便的方法：修改所有工作节点上的 <code>compute_classpath.sh</code>，以包含驱动程序的 jar 包</p>
</li>
<li><p>其它一些数据库，比如 H2, 将所有的 name 都转成大写的。你在 Spark SQL 中需要把对应的 name 转成大写形式。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ljm516.github.io/2018/11/05/spark-sparkSQL/" data-id="cjqnsmu0r002z07fycr4efcs7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/11/05/sparkTransformation-Actions/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          spark transformation 和 actions 操作
        
      </div>
    </a>
  
  
    <a href="/2018/04/19/python爬虫实战-百度搜索联想词抓取/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">python 爬虫实战之-百度联想词抓取</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/消息队列/">消息队列</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/经历/">经历</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ORM/">ORM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RocketMQ/">RocketMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/broker/">broker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/consumer/">consumer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/maven/">maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/producer/">producer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scrapy/">scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/设计模式/">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试题/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MySQL/" style="font-size: 12.5px;">MySQL</a> <a href="/tags/ORM/" style="font-size: 10px;">ORM</a> <a href="/tags/RocketMQ/" style="font-size: 15px;">RocketMQ</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/broker/" style="font-size: 10px;">broker</a> <a href="/tags/consumer/" style="font-size: 10px;">consumer</a> <a href="/tags/maven/" style="font-size: 10px;">maven</a> <a href="/tags/producer/" style="font-size: 10px;">producer</a> <a href="/tags/redis/" style="font-size: 10px;">redis</a> <a href="/tags/scrapy/" style="font-size: 17.5px;">scrapy</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/设计模式/" style="font-size: 12.5px;">设计模式</a> <a href="/tags/面试/" style="font-size: 10px;">面试</a> <a href="/tags/面试题/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/06/rocketmq之consumer消息消费者/">rocketmq之consumer消息消费者</a>
          </li>
        
          <li>
            <a href="/2019/01/03/rocketmq之producer消息发送者/">rocketmq之producer消息发送者</a>
          </li>
        
          <li>
            <a href="/2018/12/28/rocketmq之broker源码学习/">rocketmq之broker源码学习</a>
          </li>
        
          <li>
            <a href="/2018/11/05/sparkTransformation-Actions/">spark transformation 和 actions 操作</a>
          </li>
        
          <li>
            <a href="/2018/11/05/spark-sparkSQL/">spark SQL 编程入门</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 lijianmging<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>