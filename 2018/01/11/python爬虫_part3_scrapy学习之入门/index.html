<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>python爬虫_part3_scrapy框架学习之入门 | ljming的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="开启爬虫框架 scrapy 学习之旅">
<meta name="keywords" content="爬虫,scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫_part3_scrapy框架学习之入门">
<meta property="og:url" content="http://ljming.top/2018/01/11/python爬虫_part3_scrapy学习之入门/index.html">
<meta property="og:site_name" content="ljming的博客">
<meta property="og:description" content="开启爬虫框架 scrapy 学习之旅">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2018-01-11T15:52:22.888Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="python爬虫_part3_scrapy框架学习之入门">
<meta name="twitter:description" content="开启爬虫框架 scrapy 学习之旅">
  
    <link rel="alternate" href="/atom.xml" title="ljming的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ljming的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">程序人生</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://ljming.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-python爬虫_part3_scrapy学习之入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/python爬虫_part3_scrapy学习之入门/" class="article-date">
  <time datetime="2018-01-11T15:40:00.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/python/">python</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      python爬虫_part3_scrapy框架学习之入门
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>开启爬虫框架 scrapy 学习之旅</p>
<a id="more"></a>
<ul>
<li>安装</li>
</ul>
<p><code>pip install scrapy</code></p>
<h4 id="全局命令"><a href="#全局命令" class="headerlink" title="全局命令"></a>全局命令</h4><p><code>scrapy -h</code> 查看有哪些全局命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scrapy -h</span><br><span class="line"></span><br><span class="line">Scrapy 1.5.0 - no active project</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  scrapy &lt;command&gt; [options] [args]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  bench         Run quick benchmark test</span><br><span class="line">  fetch         Fetch a URL using the Scrapy downloader</span><br><span class="line">  genspider     Generate new spider using pre-defined templates</span><br><span class="line">  runspider     Run a self-contained spider (without creating a project)</span><br><span class="line">  settings      Get settings values</span><br><span class="line">  shell         Interactive scraping console</span><br><span class="line">  startproject  Create new project</span><br><span class="line">  version       Print Scrapy version</span><br><span class="line">  view          Open URL in browser, as seen by Scrapy</span><br><span class="line"></span><br><span class="line">  [ more ]      More commands available when run from project directory</span><br><span class="line"></span><br><span class="line">Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command</span><br></pre></td></tr></table></figure>
<ul>
<li>fetch 命令</li>
</ul>
<p>主要用来显示爬虫爬取的过程</p>
<p>一些参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy fetch --headers: 控制显示对应的爬虫爬取网页时候的头信息</span><br><span class="line">scrapy fetch --nolog: 控制不显示日志信息</span><br><span class="line">scrapy fetch --spider=[spiderName]: 控制使用哪个爬虫</span><br><span class="line">scrapy fetch --logfile=[filePath]: 指定存储日志信息的文件</span><br><span class="line">scrapy fetch --loglevel=[level]： 控制日志等级</span><br></pre></td></tr></table></figure></p>
<ul>
<li>runspider 命令</li>
</ul>
<p>通过该命令可以实现不依托 scrapy 的爬虫项目，直接运行一个爬虫文件（首先要写好一个 scrapy 爬虫文件）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Spider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">	name = <span class="string">'first'</span></span><br><span class="line">	allowed_domains = [<span class="string">'baidu.com'</span>]</span><br><span class="line">	start_urls = [</span><br><span class="line">		<span class="string">'http://www.baidu.com'</span></span><br><span class="line">	]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>执行 <code>scrapy runspider [.py]</code></p>
<ul>
<li>settings 命令</li>
</ul>
<p>该命令用来查看 scrapy 的配置信息</p>
<p><code>scrapy settings --get [conf_name]</code></p>
<ul>
<li>shell 命令</li>
</ul>
<p>启动 scrapy 的交互终端</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://www.baidu.com</span><br><span class="line"></span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.5</span><span class="number">.0</span> started (bot: scrapybot)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.utils.log] INFO: Versions: lxml <span class="number">4.1</span><span class="number">.1</span><span class="number">.0</span>, libxml2 <span class="number">2.9</span><span class="number">.5</span>, cssselect <span class="number">1.0</span><span class="number">.3</span>, parsel <span class="number">1.3</span><span class="number">.1</span>, w3lib <span class="number">1.18</span><span class="number">.0</span>, Twisted <span class="number">17.9</span><span class="number">.0</span>, Python <span class="number">3.6</span><span class="number">.3</span> (v3<span class="number">.6</span><span class="number">.3</span>:<span class="number">2</span>c5fed8, Oct  <span class="number">3</span> <span class="number">2017</span>, <span class="number">18</span>:<span class="number">11</span>:<span class="number">49</span>) [MSC v<span class="number">.1900</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">17.5</span><span class="number">.0</span> (OpenSSL <span class="number">1.1</span><span class="number">.0</span>g  <span class="number">2</span> Nov <span class="number">2017</span>), cryptography <span class="number">2.1</span><span class="number">.4</span>, Platform Windows<span class="number">-10</span><span class="number">-10.0</span><span class="number">.16299</span>-SP0</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.crawler] INFO: Overridden settings: &#123;<span class="string">'DUPEFILTER_CLASS'</span>: <span class="string">'scrapy.dupefilters.BaseDupeFilter'</span>, <span class="string">'LOGSTATS_INTERVAL'</span>: <span class="number">0</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.corestats.CoreStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.extensions.telnet] DEBUG: Telnet console listening on <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">53</span> [scrapy.core.engine] INFO: Spider opened</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">54</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://www.baidu.com&gt; (referer: <span class="keyword">None</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">55</span> [traitlets] DEBUG: Using default logger</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">55</span> [traitlets] DEBUG: Using default logger</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at <span class="number">0x0000018E991AAC88</span>&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http://www.baidu.com&gt;</span><br><span class="line">[s]   response   &lt;<span class="number">200</span> http://www.baidu.com&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at <span class="number">0x0000018E9B881CC0</span>&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">'default'</span> at <span class="number">0x18e9bb08550</span>&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   fetch(url[, redirect=<span class="keyword">True</span>]) Fetch URL <span class="keyword">and</span> update local objects (by default, redirects are followed)</span><br><span class="line">[s]   fetch(req)                  Fetch a scrapy.Request <span class="keyword">and</span> update local objects</span><br><span class="line">[s]   shelp()           Shell help (<span class="keyword">print</span> this help)</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br><span class="line">In [<span class="number">1</span>]: title = response.xpath(<span class="string">'/html/head/title'</span>)</span><br><span class="line">In [<span class="number">2</span>]: print(title)</span><br><span class="line">[&lt;Selector xpath=<span class="string">'/html/head/title'</span> data=<span class="string">'&lt;title&gt;百度一下，你就知道&lt;/title&gt;'</span>&gt;]</span><br><span class="line">In [<span class="number">3</span>]: exit()  <span class="comment"># 退出</span></span><br></pre></td></tr></table></figure>
<ul>
<li>startproject 命令</li>
</ul>
<p>用于创建项目 <code>scrapy startprject [project_name]</code></p>
<ul>
<li>version 命令</li>
</ul>
<p>用于显示 scrapy 版本信息 <code>scrapy verion</code> or <code>scrapy -v</code></p>
<ul>
<li>view 命令</li>
</ul>
<p>实现下载某个页面并用浏览器查看的功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">scrapy view http://news.sina.com.cn</span><br><span class="line"></span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">15</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.5</span><span class="number">.0</span> started (bot: scrapybot)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">15</span> [scrapy.utils.log] INFO: Versions: lxml <span class="number">4.1</span><span class="number">.1</span><span class="number">.0</span>, libxml2 <span class="number">2.9</span><span class="number">.5</span>, cssselect <span class="number">1.0</span><span class="number">.3</span>, parsel <span class="number">1.3</span><span class="number">.1</span>, w3lib <span class="number">1.18</span><span class="number">.0</span>, Twisted <span class="number">17.9</span><span class="number">.0</span>, Python <span class="number">3.6</span><span class="number">.3</span> (v3<span class="number">.6</span><span class="number">.3</span>:<span class="number">2</span>c5fed8, Oct  <span class="number">3</span> <span class="number">2017</span>, <span class="number">18</span>:<span class="number">11</span>:<span class="number">49</span>) [MSC v<span class="number">.1900</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">17.5</span><span class="number">.0</span> (OpenSSL <span class="number">1.1</span><span class="number">.0</span>g  <span class="number">2</span> Nov <span class="number">2017</span>), cryptography <span class="number">2.1</span><span class="number">.4</span>, Platform Windows<span class="number">-10</span><span class="number">-10.0</span><span class="number">.16299</span>-SP0</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">15</span> [scrapy.crawler] INFO: Overridden settings: &#123;&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">15</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.corestats.CoreStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.logstats.LogStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.core.engine] INFO: Spider opened</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.extensions.logstats] INFO: Crawled <span class="number">0</span> pages (at <span class="number">0</span> pages/min), scraped <span class="number">0</span> items (at <span class="number">0</span> items/min)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">16</span> [scrapy.extensions.telnet] DEBUG: Telnet console listening on <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">25</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://news.sina.com.cn&gt; (referer: <span class="keyword">None</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">26</span> [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">26</span> [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;<span class="string">'downloader/request_bytes'</span>: <span class="number">215</span>,</span><br><span class="line"> <span class="string">'downloader/request_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/request_method_count/GET'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_bytes'</span>: <span class="number">132044</span>,</span><br><span class="line"> <span class="string">'downloader/response_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_status_count/200'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'finish_reason'</span>: <span class="string">'finished'</span>,</span><br><span class="line"> <span class="string">'finish_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">1</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">3</span>, <span class="number">26</span>, <span class="number">66996</span>),</span><br><span class="line"> <span class="string">'log_count/DEBUG'</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">'log_count/INFO'</span>: <span class="number">7</span>,</span><br><span class="line"> <span class="string">'response_received_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'start_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">1</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">51050</span>)&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-01</span><span class="number">-11</span> <span class="number">23</span>:<span class="number">03</span>:<span class="number">26</span> [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>
<h4 id="项目命令"><a href="#项目命令" class="headerlink" title="项目命令"></a>项目命令</h4><p>bench, check, crawl, edit, genspider, list, parse 等</p>
<ul>
<li>bench 命令</li>
</ul>
<p>测试本地硬件性能</p>
<ul>
<li>genspider 命令</li>
</ul>
<p>创建 scrapy 文件，快速创建爬虫文件的方式。 需要在 scrapy 爬虫项目目录中，才能使用该命令。</p>
<p>可以使用 <code>scrapy genspider -l</code> 查看模板</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -l</span><br><span class="line"></span><br><span class="line">Available templates:</span><br><span class="line">  basic</span><br><span class="line">  crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br></pre></td></tr></table></figure>
<p>可以选取一个模板生成一个爬虫文件: <code>scrapy genspider basic [file_name] [url]</code></p>
<p>查看某个模板的内容 <code>scrapy genspider -d [template_name]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CSVFeedSpider</span><br><span class="line"></span><br><span class="line">class $classname(CSVFeedSpider):</span><br><span class="line">    name = <span class="string">'$name'</span></span><br><span class="line">    allowed_domains = [<span class="string">'$domain'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://$domain/feed.csv'</span>]</span><br><span class="line">    <span class="comment"># headers = ['id', 'name', 'description', 'image_link']</span></span><br><span class="line">    <span class="comment"># delimiter = '\t'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Do any adaptations you need here</span></span><br><span class="line">    <span class="comment">#def adapt_response(self, response):</span></span><br><span class="line">    <span class="comment">#    return response</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_row</span><span class="params">(self, response, row)</span>:</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#i['url'] = row['url']</span></span><br><span class="line">        <span class="comment">#i['name'] = row['name']</span></span><br><span class="line">        <span class="comment">#i['description'] = row['description']</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure>
<ul>
<li>check 命令</li>
</ul>
<p>在 scrapy 中使用合同（contract）[一种交互式的检查方式] 的方式对爬虫进行测试</p>
<p>格式： <code>scrapy check [spider_name]</code>  # 不是文件名，即不带 <code>.py</code> 后缀</p>
<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LjmSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'ljm'</span>  <span class="comment"># 是这个 name</span></span><br><span class="line">    allowed_domains = [<span class="string">'baidu.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://baidu.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scrapy check ljm</span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">Ran <span class="number">0</span> contracts <span class="keyword">in</span> <span class="number">0.000</span>s</span><br><span class="line"></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<ul>
<li>crawl 命令</li>
</ul>
<p>启动某个爬虫: <code>scrapy crawl [spider_name]</code></p>
<ul>
<li>list 命令</li>
</ul>
<p>列出当前可使用的爬虫文件：<code>scrapy list</code></p>
<ul>
<li>edit 命令</li>
</ul>
<p>直接打开对应编辑器对爬虫文件进行编辑（windows）环境会出现问题，比较实用 linux 环境。</p>
<p>在 windows 环境打开:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy edit ljm</span><br><span class="line">&apos;%s&apos; 不是内部或外部命令，也不是可运行的程序</span><br><span class="line">或批处理文件。</span><br></pre></td></tr></table></figure>
<ul>
<li>parse 命令</li>
</ul>
<p>可以实现获取指定的 url 网址，并使用对用的爬虫文件进行处理和分析。没有指定爬虫文件或处理函数，会使用默认的。</p>
<p>scrapy parse -h 查看参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Usage</span><br><span class="line">=====</span><br><span class="line">  scrapy parse [options] &lt;url&gt;</span><br><span class="line"></span><br><span class="line">Parse URL (using its spider) and print the results</span><br><span class="line"></span><br><span class="line">Options</span><br><span class="line">=======</span><br><span class="line">--help, -h              show this help message and exit</span><br><span class="line">--spider=SPIDER         use this spider without looking for one</span><br><span class="line">-a NAME=VALUE           set spider argument (may be repeated)</span><br><span class="line">--pipelines             process items through pipelines</span><br><span class="line">--nolinks               don&apos;t show links to follow (extracted requests)</span><br><span class="line">--noitems               don&apos;t show scraped items</span><br><span class="line">--nocolour              avoid using pygments to colorize the output</span><br><span class="line">--rules, -r             use CrawlSpider rules to discover the callback</span><br><span class="line">--callback=CALLBACK, -c CALLBACK</span><br><span class="line">                        use this callback for parsing, instead looking for a</span><br><span class="line">                        callback</span><br><span class="line">--meta=META, -m META    inject extra meta into the Request, it must be a valid</span><br><span class="line">                        raw json string</span><br><span class="line">--depth=DEPTH, -d DEPTH</span><br><span class="line">                        maximum depth for parsing requests [default: 1]</span><br><span class="line">--verbose, -v           print each depth level one by one</span><br><span class="line"></span><br><span class="line">Global Options</span><br><span class="line">--------------</span><br><span class="line">--logfile=FILE          log file. if omitted stderr will be used</span><br><span class="line">--loglevel=LEVEL, -L LEVEL</span><br><span class="line">                        log level (default: DEBUG)</span><br><span class="line">--nolog                 disable logging completely</span><br><span class="line">--profile=FILE          write python cProfile stats to FILE</span><br><span class="line">--pidfile=FILE          write process ID to FILE</span><br><span class="line">--set=NAME=VALUE, -s NAME=VALUE</span><br><span class="line">                        set/override setting (may be repeated)</span><br><span class="line">--pdb                   enable pdb on failure</span><br></pre></td></tr></table></figure>
<p>参数说明</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>–spider=SPIDER</td>
<td>强行指定某个爬虫文件 spider 进行处理</td>
</tr>
<tr>
<td>-a Name=VALUE</td>
<td>设置 spider 的参数，可能会重复</td>
</tr>
<tr>
<td>–pipelines</td>
<td>通过 pipelines 来处理 items</td>
</tr>
<tr>
<td>–nolinks</td>
<td>不展示提取到的链接信息</td>
</tr>
<tr>
<td>–noitems</td>
<td>不展示得到的 items</td>
</tr>
<tr>
<td>–nocolour</td>
<td>输出的结果颜色不高亮</td>
</tr>
<tr>
<td>–rules, -r</td>
<td>使用 CrawlSpider 规则去处理回调函数</td>
</tr>
<tr>
<td>–callback=CALLBACk, -c CALLBACK</td>
<td>使用 spider 中用于处理返回的响应的回调函数</td>
</tr>
<tr>
<td>–depth=DEPTH, -d DEPTH</td>
<td>设置爬行深度， 默认为1</td>
</tr>
<tr>
<td>–verbose, -v</td>
<td>显示每层的详细信息</td>
</tr>
</tbody>
</table>
<p>如： <code>scrapy parse http://www.baidu.com --spider=[spider_name]</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ljming.top/2018/01/11/python爬虫_part3_scrapy学习之入门/" data-id="cjda3sa41000yz0ged2ip3xj8" class="article-share-link">Partager</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scrapy/">scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫/">爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/01/15/python爬虫实战-google_related_search/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Récent</strong>
      <div class="article-nav-title">
        
          python 爬虫实战之-google related search
        
      </div>
    </a>
  
  
    <a href="/2018/01/08/python爬虫_part2_正则/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">python 爬虫学习 part2-正则表达式</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Catégories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/经历/">经历</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Mot-clés</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ORM/">ORM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/maven/">maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scrapy/">scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/设计模式/">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试题/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nuage de mot-clés</h3>
    <div class="widget tagcloud">
      <a href="/tags/MySQL/" style="font-size: 13.33px;">MySQL</a> <a href="/tags/ORM/" style="font-size: 10px;">ORM</a> <a href="/tags/Spark/" style="font-size: 16.67px;">Spark</a> <a href="/tags/maven/" style="font-size: 10px;">maven</a> <a href="/tags/redis/" style="font-size: 10px;">redis</a> <a href="/tags/scrapy/" style="font-size: 16.67px;">scrapy</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/设计模式/" style="font-size: 13.33px;">设计模式</a> <a href="/tags/面试/" style="font-size: 10px;">面试</a> <a href="/tags/面试题/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/02/05/spark-sparkSQL/">spark SQL 编程入门</a>
          </li>
        
          <li>
            <a href="/2018/01/31/python-small-skills/">python 中的一些小技巧合集</a>
          </li>
        
          <li>
            <a href="/2018/01/21/python爬虫_part4_scrapy框架学习之进阶/">python爬虫_part4_scrapy框架学习之进阶</a>
          </li>
        
          <li>
            <a href="/2018/01/16/python-excel/">python 处理 execl 文件</a>
          </li>
        
          <li>
            <a href="/2018/01/15/python爬虫实战-google_related_search/">python 爬虫实战之-google related search</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 lijianmging<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>